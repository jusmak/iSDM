---
title: "SDM exploration - Hummingbirds"
author: "Jussi MÃ¤kinen"
date: "12/7/2020"
output:
  html_document: default
  pdf_document: default
---

## Objective of the exploration

In this exploration study I build and estimate a SDM with a hierarchical Bayesian inference. I apply a Poisson point process as the observation model. The model is compared to a MAXENT model, which is based on a maximum likelihood estimation with lasso reguralization parameters in the inference.

The document has three parts:
1. Constructing a data set for presence-only observations.
2. Structure and fitting of a hierarchical Bayesian model.
3. Accuracy of the fitted model and predictive maps.

The example study here is for the marvelous spatuletail (Loddigesia mirabilis).

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

.wd <- 'C:/Users/OMISTAJA/Documents/Hummingbird_project'
.seed <- 5326
set.seed(.seed)
lib_path <- .libPaths()[1]
suppressPackageStartupMessages({
  library(raster, lib.loc=lib_path)
  library(glmnet, lib.loc=lib_path)
  library(pracma, lib.loc=lib_path)
  library(ppmlasso, lib.loc=lib_path)
  library(rstan, lib.loc=lib_path)
  library(bayesplot, lib.loc=lib_path)
  library(rstanarm, lib.loc=lib_path)
  library(ggplot2, lib.loc=lib_path)
  library(dismo, lib.loc=lib_path)
  library(dplyr, lib.loc=lib_path)
  library(loo, lib.loc=lib_path)
  library(caret, lib.loc=lib_path)
  library(pROC, lib.loc=lib_path)
  library(flexclust, lib.loc=lib_path)
  library(RColorBrewer, lib.loc=lib_path)
})

#Set SDM parameters
species_list = c("Loddigesia_mirabilis", "Ocreatus_underwoodii") #
#option for spatially thin occurrence records
thinning = FALSE

#set the number of quadrature points in the coarse scale grid
target_n_obs = 50000

#set the number of samples taken randomly from the posterior distribution
#for computing the posterior predictive distribution
n_samp = 500

#whether weights are assigned according to the area related to a quadrature point
weights_area <- FALSE

.species <- species_list[1]

#set scale out according to the target_n_obs
domain_temp <- stack(paste(.wd,"/Data/Domains/", .species, ".tif", sep = ''))
scale_out <- round(sqrt(sum(!is.na(values(domain_temp[[1]])))/target_n_obs))

m_category = 1

#model fits for each species represent either category 1, 2 or 3

if (m_category == 1) {
  #inference types
  HB_inf = TRUE
  glmnet_inf = TRUE
  ppml_inf = FALSE
  #combine likelihoods for presence-only and presence-absence data
  lik_comb = FALSE
} 
if (m_category == 2) {
  #inference types
  HB_inf = FALSE
  glmnet_inf = FALSE
  ppml_inf = TRUE
  #combine likelihoods for presence-only and presence-absence data
  lik_comb = TRUE
} 
if (m_category == 3) {
  HB_inf = TRUE
  glmnet_inf = FALSE
  ppml_inf = FALSE
  #combine likelihoods for presence-only and presence-absence data
  lik_comb = TRUE
}

#get data
source(paste(.wd, "R_code/Get_data.R", sep = '/'))
data <- Get_data(.wd, .species, thinning, target_n_obs, weights_area)

#get model fits
source(paste(.wd, "R_code/RunInference_v1.R", sep = '/'))
model_fits <- RunInference_v1(.wd, .species, data, HB_inf, glmnet_inf, ppml_inf,
                              lik_comb, thinning, target_n_obs, m_category)

```
## 1. Data

### 1.1. Species observations

```{r observations, echo=FALSE}

  #visualize the species observations and the offsets
  #load raster
  raster_cov_temp <- stack(paste(.wd,"Data/Environment/Chelsa_SA.tif", sep = '/'))
  
  #load geographic extent
  domain_temp <- stack(paste(.wd,"/Data/Domains/", .species, ".tif", sep = ''))
  
  raster_cov_temp <- crop(raster_cov_temp, domain_temp)
  values(raster_cov_temp)[is.na(values(domain_temp)[,2]),] <- NA

  #rescale raster
  raster_cov_temp <- raster::aggregate(raster_cov_temp, fact = scale_out)

  #find cells which have all covariate values
  cov_df <- as.data.frame(raster_cov_temp, na.rm = FALSE)
  rows_na <- apply(cov_df,1,anyNA)
  ind_na <- which(rows_na==FALSE)

  #create a background
  background <- raster_cov_temp$Chelsa_SA.1
  sp_occ <- data$training_data$coordinates[data$training_data$response==1,]
  sp_occ <- cbind(sp_occ[,1]*1000 + data$training_data$min_coordinates[1],
                  sp_occ[,2]*1000 + data$training_data$min_coordinates[2])
  
  raster_cov_temp <- stack(paste(.wd,"Data/Environment/Chelsa_SA.tif", sep = '/'))
  extent_temp <- c(-4e6, 5.5e6, -7.5e6, 2e6)
  raster_cov_temp <- crop(raster_cov_temp, extent_temp)
  background_domain <- raster_cov_temp$Chelsa_SA.1
  
  
  mycol <- rgb(0, 0, 255, max = 255, alpha = 75, names = "blue50")
  col_temp <- colorRamp("gray")
  mycol_2 <- col_temp(1)/255

  cm.cols1.1=function(x,bias=1) { colorRampPalette(c('grey90','grey90','grey90','grey90','grey60','grey60','grey60','steelblue4','steelblue1','gold','gold','red1','red1','red4','red4'),bias=bias)(x)
  }
  col_pal <- cm.cols1.1(50)
  
  par(mfrow = c(1,2), cex.main = .7, mai = c(0.1,0.1,0.2,0.1))
  plot(background_domain, axes = F, box = FALSE, main = "Study domain", col = mycol_2, legend = FALSE)
  plot(background, axes = F, box = FALSE, legend = FALSE, add = TRUE, col = mycol)
  
  plot(background, axes = F, box = FALSE, legend = FALSE, main = paste("Observations, n_obs = ", as.character(sum(data$training_data$response)), sep = ''),
       col = mycol)
  points(sp_occ, cex = 1, pch = 20, col = "gray10")
  legend(x = 2.42e5, y = -2.6e6,  legend = "occurrence", col = "gray10", pch = 20)

```

Fig. 1. Location of study domain and locations of occurrence observations

### 1.2. Adding quadrature points in the study domain
Quadrature points refer to background points, which are used as a technical mean to compute the likelihood of a presence-only data set.

``` {r quadrature scheme, echo = FALSE}
  sp_occ <- data$training_data$coordinates
  sp_occ <- cbind(sp_occ[,1]*1000 + data$training_data$min_coordinates[1],
                  sp_occ[,2]*1000 + data$training_data$min_coordinates[2])
  y_lim <- c(-2.4e6, -2.38e6)
  x_lim = c(2.3e5, 2.5e5)
  ind_temp <- which(sp_occ[,1] > x_lim[1] & sp_occ[,1] < x_lim[2] &
                      sp_occ[,2] > y_lim[1] & sp_occ[,2] < y_lim[2])

  # move presence points slightly
  #sp_occ[data$training_data$response==1,1] <- sp_occ[data$training_data$response==1,1] + 250
  #sp_occ[data$training_data$response==1,2] <- sp_occ[data$training_data$response==1,2] + 250

  #color presence points
  col_points <- ifelse(data$training_data$response[ind_temp] == 1, "red3", "gray10")
  cex_points <- rep(.5,length(ind_temp))
  cex_points[data$training_data$response[ind_temp] == 1] <- 2
    
  plot(background, legend = FALSE, main = paste("Quadrature scheme, n_obs = ", as.character(sum(data$training_data$response)), sep = ''),
       col = mycol, xlim = x_lim, ylim = y_lim)
  points(sp_occ[ind_temp,], cex = cex_points, pch = 20, col = col_points)
  legend(x = 2.42e5, y = -2.396e6,  legend = c("occurrence", "quadrature point"), col = c("red3", "gray10"), pch = 20)


```

Fig. 2. A close look at the locations of occurrence points and quadrature points.

Following point process literature, occurrence points were moved to the location of their closest quadrature point. This allows to fit the model with Poisson regression. Quadrature points are sampled more intensively in a 5 km radius from the occurrence locations. This ensures that the spatial accuracy of occurrence records is maintained in the original level.
Quadrature points are sampled with the same spatial resolution as the environmental covariates in the vicinity of the occurrence records (around 1 km^2). Further away from the occurrence records the sampling intensity is around 25 km^2.

### 1.3. Sampling weights

The varying sampling intensity of the quadrature points is accounted for with a sampling weight. The weights relate the quadrature points to the areas that the points are assumed to cover. Sampling weights improve the likelihood estimate of the data in a similar fashion as does increasing the number of quadrature points.

``` {r sampling weights, echo = FALSE, message=FALSE, results = FALSE}
  sp_occ_temp <- sp_occ[ind_temp,]


  #color areas
  rbPal <- colorRampPalette(c('green','blue'))
  col_rb <- rbPal(2)
  col_poly <- ifelse(data$training_data$weights[ind_temp] == 25, col_rb[1], col_rb[2])
  
  polygons_temp <- list()

  #get the actual resolution  
  res_domain <- res(domain_temp)/1000
  
  for (i in 1:nrow(sp_occ_temp)) {
    d <- ifelse(data$training_data$weights[ind_temp[i]] != 25, 500, 2500)
    poly_x <- c(sp_occ_temp[i,c(1,1)] - d*res_domain[1], sp_occ_temp[i,c(1,1)] + d*res_domain[1])
    poly_y <- c(sp_occ_temp[i,2] - d*res_domain[2], sp_occ_temp[i,c(2,2)] + d*res_domain[2], sp_occ_temp[i,2] - d*res_domain[2])
    polygons_temp[[i]] <- list(xx = poly_x, yy = poly_y, col = col_poly[i])
  }
  
  plot(background, legend = FALSE, main = paste("Sampling weights, n_obs = ", as.character(sum(data$training_data$response)), sep = ''),
       col = mycol, xlim = x_lim, ylim = y_lim)
  lapply(polygons_temp,function(x){polygon(x$xx,x$yy,col=x$col,border="gray")})
  points(sp_occ_temp, cex = cex_points, pch = 20, col = col_points)
  
  legend(x = 2.42e5, y = -2.3925e6,  legend = c("occurrence", "quadrature point", "weight = 1", "weight = 25"), col = c("red3", "gray10", "blue", "green"), pch = c(20,20,15,15))


```


Fig. 3. Sampling weights are one for the blue cells and 25 for the green cells. Occurrence records and the quadrature points which are sampled from the occurrence cells have a weight of 1/2. By the way, there is a small bug in setting the grid system for coarse scale quadrature points since cells overlap slightly.


### 1.4. Offsets

SDMs were fitted by using offset information based on expert range maps and prior information about species response to elevation.

```{r offsets, echo = FALSE}

par(mfrow = c(1,2), cex.main = .7, mai = c(0.1,0.1,0.2,0.1))
values(background)[ind_na] <- log(data$pred_data$offset[[2]][,1])
plot(background, axes = F, box = FALSE, main = "Expert range map", xaxt='n', yaxt='n', col = col_pal, zlim = quantile(log(data$pred_data$offset[[2]][,1]),c(0,.999)), legend.args=list(text="log(lambda)", line=.2 ,side=3, cex=.5), axis.args=list(cex.axis=.5))


values(background)[ind_na] <- 1
plot(background, axes = F, box = FALSE, legend = FALSE)
values(background)[ind_na] <- log(data$pred_data$offset[[3]][,2])
plot(background, axes = F, box = FALSE, main = "Elevation offset", xaxt='n', yaxt='n', col = col_pal,zlim = quantile(log(data$pred_data$offset[[2]][,1]),c(0,.999)), legend.args=list(text="log(lambda)", line=.2 ,side=3, cex=.5), axis.args=list(cex.axis=.5), add = T)


```

Fig. 4. Offsets of expert range map and elevation. Log(lambda) refers to the expected log intensity (=areal density) of the species based on the prior information.

### 1.5. Covariates
```{r covariates, echo = FALSE}
names(raster_cov_temp) <- c('Mean temp.', 'Mean diurnal range', 'Precipitation', 'Precip. seasonality')
extent_temp <- extent(background)

plot(raster_cov_temp, xlim = c(extent_temp@xmin, extent_temp@xmax), ylim = c(extent_temp@ymin, extent_temp@ymax))

```

Fig. 5. Mappings of model covariates.


## 2. Model structure

### 2.1. Poisson point process

Generally in a SDM context, a point process models jointly the total number and spatial locations of species occurrence points. Here, I applied inhomogeneous Poisson point process, which assumes that locations of species occurrence points are independently distributed and their sum over the study area is Poisson distributed. The locations of species occurrence points depend on the species intensity (=areal density) which varies spatially.
In this exploration I conditioned the species intensity on the environmental covariates.
Likelihood of a Poisson point process is defined as
$$ l(\lambda; s_P) = \sum_{i=1}^{m} log(\lambda(s_i)) - \int_A \lambda(s)\mathrm{d}s,$$
where lambda is for the species intensity, s a spatial location and <em>A</em> is for the study area. However, the integral over the study area cannot be computed and that needs to be approximated by discretizing the study area as we saw in the figures 2 and 3. High number of quadrature points (> 10000) and sampling weights related to each quadrature point improve estimation accuracy of the model. Given the quadrature scheme, a Poisson point process model can be fitted as a normal Poisson regression, also known as log-linear regression.
Species intensity is modeled as a log linear model so that:
$$ log(\lambda) = \alpha + \sum_{k=1}^{K} x_k^T*\beta_k, $$
where alpha is for the model estimate,  beta is for the linear weight related to each covariate and K is the number of covariates and their transformations, such as products and second order polynomials. The offset information is included in the log-linear model as a fixed variable.

To condition the inference on the offsets we divide lambda by the offset so that
$$ log(\lambda/\lambda_{offset}) = log(\lambda) - log(\lambda_{offset}). $$
This changes the log-linear model to
$$ log(\lambda/\lambda_{offset}) = \alpha + \sum_{k=1}^{K} x_k^T*\beta_k - log(\lambda_{offset}), $$
Here offset intensity is scaled to sum to the number of species observations over the study area. To avoid changes in estimates of model intercept we scaled the offset to sum to zero over the study area. We did this by subtracting average species intensity over the study area from the offset intensity
$$ log(\lambda_{offset_{scaled}}) = log(\lambda_{offset_{raw}}*A/n), $$
where A is the study area and n is the number of species observations.

### 2.2. Inference
I fitted the model to estimate posterior distributions of the unknown parameters. Each parameter was assigned an uninformative prior so that:
$$ \alpha \sim N(0,10) $$
$$ \beta_k \sim N(0,10) $$
The models were fitted with STAN-software applying No-U-Turn Markov Chain Monte Carlo sampling. I derived in total 5000 samples for each model and discarded 2000 first samples as a burn-in.

## 3. Results

### 3.1. Posterior predictive distributions
The posterior distributions of model parameters were used for predicting species log-intensity throughout the study area. Using 3000 samples from parameters' posterior distribution allowed me to compute a posterior predictive distribution to each prediction cell. The predictive distribution could be used to compute the mean and standard deviation of the prediction. Here Bayesian inference provides good tools to map the uncertainty related to the model and expressed as a probabilistic distribution.

```{r posterior predictive distribution, echo = FALSE}
  #compute thresholds
  set.seed(1)
  beta_samp <- extract(model_fits$HB_model[[1]])
  ind_samples <- sample(1:nrow(beta_samp$alpha), n_samp)

  #matrix of covariate values in prediction cells  
  pred_matrix <- cbind(data$pred_data$covariates, data$pred_data$covariates^2)

  par(mfrow = c(1,2), cex.main = .7, mai = c(0.1,0.1,0.2,.1))
  i=2 #expert offset for fitting
  j=i #expert offset for predicting
  offset_temp <- log(data$pred_data$offset[[j]][,1]) + log(data$pred_data$offset[[j]][,2])  
  
  beta_samp <- extract(model_fits$HB_model[[i]])

  pred <- matrix(rep(beta_samp$alpha[ind_samples], nrow(pred_matrix)), nrow = nrow(pred_matrix), byrow = TRUE) + pred_matrix %*% t(beta_samp$beta[ind_samples,])

  mean_pred <- exp(pred + offset_temp)
  mean_pred <- apply(mean_pred, 1, mean)
  values(background)[ind_na] <- log(mean_pred)
  plot(background, axes = F, box = FALSE, legend = FALSE, xaxt='n', yaxt='n', main = 'Posterior predictive mean', col = col_pal[1])
  plot(background, axes = F, box = FALSE, main = 'Posterior predictive mean', xaxt='n', yaxt='n', col = col_pal, zlim = quantile(log(mean_pred), c(.2, 1)), add = TRUE,
legend.args=list(text="log(lambda)", line=0 ,side=3, cex=.5), axis.args=list(cex.axis=.5),
smallplot= c(.82,.84,.03,.2))
      
  ##sd
  mean_pred <- pred + offset_temp
  mean_pred <- apply(mean_pred, 1, function(x) sd(x))
  values(background)[ind_na] <- mean_pred
  l_pred <- values(background)
  
  #set color scales
  in.max=quantile(l_pred,prob = .99, na.rm=T)
  in.min=min(l_pred,na.rm=T)
  
  thres_temp <- quantile(l_pred, prob = .25, na.rm=T)
  
  tmp=quantile(l_pred[l_pred>=thres_temp],seq(0,1,length=50),na.rm=T)
  hb.breaks=c(seq(in.min,thres_temp,length=25),tmp)
  values(background)[values(background)>in.max] <- in.max
  mean.col.brk <- hb.breaks
  
  col.brk=data.frame(cols=c(grey(seq(.8,.2,length=24)),
                            colorRampPalette(c("steelblue4", "steelblue1","gold","gold3", "red1", "red4"))(50),NA),
                     breaks=mean.col.brk,stringsAsFactors=F)
  col.brk=col.brk[!duplicated(col.brk$breaks),]
  
  #make sure the top color is red
  col.brk$cols[(nrow(col.brk)-1)]='#8B0000'
  
  #avoid too many legend labels
  lab.breaks.tmp=col.brk$breaks
  keep=round(seq(2,length(lab.breaks.tmp)-1,length=3))
  axis.labs=lab.breaks.tmp[keep]
  
  plot(background, axes = F, box = FALSE, main = 'Posterior predictive uncertainty', xaxt='n', yaxt='n',
       zlim = c(col.brk$breaks[1],col.brk$breaks[nrow(col.brk)]), col=col.brk$cols[-nrow(col.brk)],
       legend.args=list(text="sd", line=.2,side=3,cex=.7*.7),
       axis.args=list(at=axis.labs, labels=round(axis.labs,2),cex.axis=.8*.7),
       breaks=col.brk$breaks, smallplot= c(.72,.74,.03,.2))
      
  
```

Fig. 6. Posterior predictive mean and uncertainty. The model is fitted by using expert range map as an offset in model fitting and predicting.

### 3.2. Model comparison
Models were evaluated by using two metrics, AUC and log posterior predictive density (LPPD). AUC is commonly applied in SDMs and LPPD is a mostly applied in Bayesian model comparison. LPPD is derived by computing the log-likelihood of a test point using each sample from the posterior distribution. Mean log-likelihood is computed over samples and test point's mean log-likelihoods are summed.
Model validation were conducted by using three different validation strategies:
- using all data for model fitting and testing model on the training data
- conducting five-fold cross-validation where occurrence data points are split to five sets and each set is used one at a time as a test data set. The test fold is left out from the model fitting. Quadrature scheme is held constant when changing between folds.
- using species inventory data as a test data.

